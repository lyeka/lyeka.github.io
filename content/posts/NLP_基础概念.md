+++
date = '2025-12-19T18:33:15+08:00'
title = 'NLP 基础概念'
tags = ['NLP']
+++

# 《第一章 NLP 基础概念》

---

## 一、这个内容真正解决的是什么问题？

如果不先把“语言如何被计算机表示、以及理解/生成被拆成哪些可计算的子问题”想清楚，人最容易在两处判断上翻车：一是把 NLP 误当成“查词典+套规则”的工程拼装，低估歧义与语境；二是把模型效果归因到“模型更大/更深”而忽略文本表示与任务设定才是性能上限的决定因素。

---

## 二、最容易产生误解的地方

* **误解一：NLP 就是把句子翻译成向量，剩下全靠模型学**
  诱因在于深度学习端到端的叙事很强，让人以为表示只是“喂进去就行”。
  代价是忽视分词/子词、标签体系、训练数据分布等前置选择，导致同一模型在不同预处理与标注规范下表现差异巨大，却误判为“模型不行”。

* **误解二：中文分词是可有可无的清洗步骤**
  诱因在于子词切分与字符级建模的普及，让人以为“分不分都一样”。
  代价是把“词边界不稳定”带来的连锁误差传给词性标注、实体识别、句法分析等任务，最终在信息抽取与检索类应用中出现系统性偏差。

* **误解三：统计方法已经过时，深度学习完全替代了它**
  诱因是近年来预训练模型在榜单上的优势过于显眼。
  代价是在资源受限或可解释性敏感的场景里，放弃 n-gram、TF-IDF、线性模型等强基线，反而让系统变慢、变贵、变难调，还不一定更稳。

* **误解四：词向量天然“懂语义”，所以能直接推理**
  诱因是“相似词向量更近”的直观效果很迷人。
  代价是把“共现相关”当成“逻辑关系”，在抽象概念、讽刺隐喻、长距离依赖上高估模型能力，产生看似合理但方向错误的输出。

---

## 三、支撑正确理解的核心机制

1. **把语言处理拆成“任务”，才能把智能落到可评估的接口上**

* 直观理解：语言很大，但系统通常只需要完成一件具体事，比如“找出人名地名”或“判断这段话属于哪类”。
* 专业表述：中文分词、子词切分、POS、分类、NER、关系抽取、摘要、翻译、问答等构成常用任务谱系，它们对应不同的输入输出形式与评价指标。
* 在整体中的位置：这是问题定义层，决定了训练数据怎么标、模型输出如何落地。

2. **文本表示决定了模型“能看见什么信息”**

* 直观理解：同一句话，用稀疏的 one-hot 看起来像“只知道出现过哪些词”，用上下文化表示则更像“知道这个词在这里是什么意思”。
* 专业表述：从 VSM/TF-IDF 的高维稀疏表示，到 Word2Vec 的静态稠密词嵌入，再到 ELMo 这类上下文化词表示，表示能力逐步从“词频共现”走向“语境条件化”。
* 在整体中的位置：这是能力上限层，直接影响分类、抽取、生成等下游任务的可达性能与资源成本。

3. **从规则到统计再到深度学习，是对“歧义与泛化”难题的三次应对**

* 直观理解：规则法像人工写语法书；统计法像从大量例子里总结规律；深度学习像用更强的函数逼近去吸收更复杂的模式。
* 专业表述：符号主义强调形式规则与逻辑范式；统计方法用概率建模替代手写规则；深度学习用 RNN/LSTM/注意力机制等在更大语料上学习表示与映射。
* 在整体中的位置：这是方法演进层，解释“为什么今天的系统长这样”。

4. **语言模型的核心是“给序列赋概率”，但建模视野受限会带来结构性缺口**

* 直观理解：n-gram 只看前面很短的几个词，所以擅长局部搭配，不擅长跨句、跨段推断。
* 专业表述：基于马尔可夫假设，n-gram 用条件概率链式规则计算句子概率；N 增大带来参数爆炸与数据稀疏。
* 在整体中的位置：这是生成与预测的基础层，为纠错、翻译、检索等提供可计算的概率框架。

5. **上下文化表示解决“一词多义”，代价是训练与推理成本显著上升**

* 直观理解：“苹果”在科技新闻与水果店里不是同一个意思，上下文能把它区分开。
* 专业表述：ELMo 通过双向 LSTM 语言模型预训练，再在下游任务中提取特征以补充表示，实现从静态到动态词向量的跃迁。
* 在整体中的位置：这是表示升级层，提升语义贴合度，同时引入计算资源与工程复杂度的约束。

---

## 四、关键术语与概念对齐

* **NLP**：让计算机理解、解释与生成自然语言的一组技术与研究方向，不等于单一模型或单一任务。
* **中文分词（CWS）**：把连续中文字符序列切分为词序列的过程，不等于子词切分。
* **子词切分（Subword Segmentation）**：把词进一步拆成更小单位以缓解未登录词与稀疏性问题，常见方法含 BPE、WordPiece、Unigram、SentencePiece，不等于“按字切”。
* **词性标注（POS Tagging）**：为词分配词类标签以支撑句法与语义分析，不等于实体识别。
* **实体识别（NER）**：识别并分类人名、地名、组织、日期等实体，不负责给出实体之间关系。
* **关系抽取（RE）**：从文本中识别实体间语义关系（如“创始人”），不等于知识库推理。
* **向量空间模型（VSM）/TF-IDF**：以稀疏高维向量表达文本并计算相似度的经典框架，不包含词序与深层语境。
* **Word2Vec**：学习静态稠密词向量的嵌入方法，主要架构为 CBOW 与 Skip-Gram，不等同于上下文化表示。
* **ELMo**：通过双向语言模型得到上下文化词表示的预训练式方法，不等于 BERT 但都属于“利用大语料预训练再服务下游任务”的思路。
* **n-gram 语言模型**：用马尔可夫假设近似建模序列概率的统计语言模型，不等于神经网络语言模型。

---

## 五、结构全景层：原文笔记

* **NLP 被定义为让计算机理解、解释和生成人类语言的技术，并被描述为人工智能的重要分支与活跃研究方向。**

  * NLP 试图模拟人类对语言的认知与使用过程，并融合计算机科学、人工智能、语言学、心理学等学科知识。
  * NLP 的目标被表述为打破人类语言与计算机语言之间的障碍，从而实现更自然的交流与互动。

* **NLP 的任务谱系覆盖从基础预处理到语义理解与生成的一系列问题。**

  * 文本处理任务被列举为中文分词、子词切分、词性标注、文本分类、实体识别、关系抽取、文本摘要、机器翻译、自动问答等。
  * 这些任务被强调不仅处理表层结构，还要触及语义、语境、情感、文化等深层因素。

* **NLP 的发展被概括为从规则方法到统计学习，再到机器学习与深度学习的多次技术革新。**

  * 早期阶段被描述为依赖字典查找与词序规则的简单机器翻译系统，并提到图灵测试与生成语法理论对研究的影响。
  * 1970–1990 年代被描述为符号主义与统计方法并行发展，且 1980 年代统计模型开始取代复杂的手写规则。
  * 2000 年代至今被描述为深度学习推动进展，提到 RNN、LSTM、注意力机制的应用，以及 Word2Vec（2013）、BERT（2018）与基于 Transformer 的 GPT-3 等模型带来的变化。

* **中文分词被定位为中文 NLP 的首要步骤，因为中文文本缺少英文那样的显式分隔符。**

  * 分词的目标被描述为把连续中文文本切分为有意义的词汇序列，并给出中英文切分示例。
  * 分词错误被展示为会拆散地名或造成边界混乱，并被强调会直接影响词性标注、实体识别、句法分析等后续流程效果。

* **子词切分被描述为缓解词汇稀疏与未见词问题的常见预处理技术。**

  * 子词切分被说明能把词分解为更小单位，从而在遇到罕见词时仍可借助已知子词理解或生成。
  * 常见方法被列为 BPE、WordPiece、Unigram、SentencePiece，并用 “unhappiness → un / happi / ness” 说明其能利用前缀、词根与后缀提供语义线索。

* **词性标注被定义为为每个单词分配词性标签，并被强调对句子结构理解与高级任务很关键。**

  * 英文例句 “She is playing the guitar in the park.” 被逐词标注为代词、动词、限定词、名词、介词与标点等类别。
  * 词性标注常用模型被列为 HMM、CRF，以及基于深度学习的 RNN 与 LSTM，并被描述为通过学习标注数据来预测新句子词性。

* **文本分类被定义为把文本自动分配到一个或多个预定义类别，并被描述为广泛用于情感分析、垃圾邮件检测、新闻分类与主题识别。**

  * 新闻分类示例展示了体育、政治、科技三类文本到类别的映射。
  * 成功关键被归结为选择合适特征表示与分类算法，并依赖高质量训练数据；神经网络方法被提到能捕捉复杂模式与语义信息以提升性能。

* **实体识别被定义为识别文本中具有特定意义的实体并分类到人名、地点、组织、日期等类别。**

  * 示例句中输出了人名、地名与日期等实体-类型对，并被说明可帮助系统理解关键元素及其属性。
  * 实体识别被指出对信息提取、知识图谱、问答系统与推荐等应用重要，且精度与效率随 NLP 发展不断提高。

* **关系抽取被定义为从文本中识别实体之间的语义关系，并被说明对理解文本与构建知识图谱具有意义。**

  * 示例 “比尔·盖茨是微软公司的创始人。” 被抽取为（比尔·盖茨，创始人，微软公司）的三元组形式。
  * 关系类型被列为因果、拥有、亲属、地理位置等，并被强调为后续知识图谱与问答系统提供支持。

* **文本摘要被定义为生成简洁准确的摘要，并区分为抽取式与生成式两类。**

  * 抽取式摘要被描述为直接选取关键句子或短语，优点是信息来自原文而准确性较高，但可能不够流畅。
  * 生成式摘要被描述为在理解深层含义基础上重组与改写，通常需要更复杂模型如注意力机制的 Seq2Seq；并给出“天问一号”新闻的抽取式与生成式摘要示例。

* **机器翻译被定义为将源语言自动翻译成目标语言，并强调不仅词汇转换还要传达语义、风格与文化背景。**

  * 示例把“今天天气很好。”翻译为 “The weather is very nice today.”，并说明长文本更复杂、挑战更大。
  * 改进方向被提到包括基于神经网络的 Seq2Seq 与 Transformer 等模型以学习源/目标语言之间的复杂映射。

* **自动问答被定义为理解自然语言问题并从数据源自动给出答案，并被划分为检索式、知识库与社区问答三类。**

  * 检索式问答被描述为从大量文本中检索答案，知识库问答依赖结构化知识库，社区问答依赖用户生成问答数据。
  * 系统构建被说明涉及信息检索、文本理解、知识表示与推理，并强调随着算法改进在准确性与适用范围上持续提升。

* **文本表示被强调为把自然语言转为计算机可处理形式的基础工作，并被描述为直接影响 NLP 系统质量与性能。**

  * 文本表示被说明要把字/词/短语/句子等单位及其关系结构转成向量、矩阵等数据结构，同时兼顾语义保留与计算/存储效率。
  * 词向量相关内容提到向量空间模型（VSM）以特征项维度表示文本并用 TF、TF-IDF 等计算权重，可用于相似度、分类与信息检索；并指出其稀疏性、维数灾难与独立性假设忽略词序与上下文等局限。
  * VSM 的 one-hot 示例展示在 16384 维词表下句子仅 5 个位置为 1 的高稀疏率，并解释词汇表与 one-hot 映射关系。
  * 语言模型相关内容介绍 n-gram 基于马尔可夫假设，用条件概率链式规则计算句子概率，优点是简单实用但 N 增大带来稀疏与长程依赖缺失。
  * Word2Vec 被描述为 2013 年提出的词嵌入技术，通过上下文学习稠密低维向量，包含 CBOW 与 Skip-Gram 两种架构，并指出其难以捕捉长距离依赖与整体关系。
  * ELMo 被描述为从静态到动态词向量的转变，引入预训练思想，采用双向 LSTM，两阶段为语言模型预训练与下游提取词向量特征，并指出其训练时间长、资源消耗大等问题。
  * 参考文献被列为 Word2Vec、BERT、Attention Is All You Need、主题建模与过滤、ELMo、VSM、中文文本表示综述与中文信息处理发展报告等条目。

---

## 六、如何把这个内容讲清楚给别人？

最值得慢下来讲的是“文本表示为什么是核心”：同一句话，模型能否抓住语义、能否在未见词上泛化、能否区分一词多义，往往先由表示方式决定，再由模型结构放大或削弱。

可以快速带过的是“任务清单的枚举”：让对方先形成“任务=明确输入输出”的观念即可，细节差异放到具体应用时再补，不会影响主干理解。

最小示例（只用一句话贯穿机制）：

> 句子：**“雍和宫的荷花开得很好。”**
>
> * 如果做 **中文分词**，系统要先决定“雍和宫”应当作为整体而不是被拆散。
> * 如果做 **实体识别**，系统要进一步把“雍和宫”标成地名/景点类实体。
> * 如果做 **文本分类**（例如“旅游/非旅游”），这些表示与抽取到的关键信息会直接影响最终类别判断。

这三个动作看似不同，背后共用一条主线：先把文本变成可计算的表示，再把目标问题约束成可评价的任务输出。

---

## 七、实践与使用视角的结论

* 适合用在什么场景

  * 海量文本需要自动处理与结构化：检索、推荐、舆情与内容审核等。
  * 需要从非结构化文本抽取关键信息：实体、关系、事件要素抽取与知识图谱构建。
  * 需要生成或改写文本：摘要、翻译、问答与对话等。

* 不适合用在什么场景

  * 数据极少且标注口径不稳定时，追求复杂模型往往不如先把任务与标签定义清楚。
  * 需要严格逻辑证明或强可控推理链的任务，单靠语言模型输出容易出现“看起来合理但不受约束”的结论。
  * 计算资源、延迟、成本强约束而又缺少收益评估的场景，直接上大模型容易得不偿失。

* 最常见、也最危险的错误用法是什么

  * 把“分词/子词/标注规范”当成无关紧要的细节，导致线上线下分布不一致而性能崩塌。
  * 把向量相似当成语义等价，把相关性当成因果或事实，从而在抽取与问答里制造错误信息。
  * 只看单一指标（如准确率或 BLEU）而忽略任务目标（可解释、可控、可追溯），让系统在关键边界条件下失效。

最低使用原则：**先把任务的输入输出、标注口径与文本表示定稳，再谈模型结构与规模。**

---

## 八、最终总结

* NLP 的核心不是“让模型读文字”，而是把语言问题拆成可定义、可评估的任务。
* 文本表示决定模型能利用的信息类型，进而决定下游任务的性能上限与稳定性。
* 规则、统计、深度学习三条路径分别在可控性、泛化能力与表示学习上做权衡。
* 分词/子词、词性、实体、关系、摘要、翻译、问答等任务相互衔接，前置误差会级联放大。
* 把语境与多义性纳入表示（如 ELMo 的上下文化思路）能提升理解力，但会带来更高的训练与推理成本。
