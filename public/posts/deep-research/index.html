<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta
    name="description"
    content="Author: Lyeka <augustr2017@163.com>"
  />
  <title>
    
      Lyeka
      | Deep Research 项目研究
    
  </title>
  <meta name="author" content="map[email:augustr2017@163.com name:Lyeka]" />
  <meta name="description" content="" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  <link
    rel="icon"
    href="/lyeka-blog/favicon_io/favicon.ico"
    type="image/x-icon"
  />
  <link
    rel="icon"
    href="/lyeka-blog/favicon_io/favicon-16x16.png"
    size="16x16"
    type="image/png"
  />
  <link
    rel="icon"
    href="/lyeka-blog/favicon_io/favicon-32x32.png"
    size="32x32"
    type="image/png"
  />

  <link
    rel="preload"
    type="text/css"
    href="/lyeka-blog/css/rose-pine.min.css"
    integrity=""
    as="style"
    onload="this.onload=null;this.rel='stylesheet'"
  />
  <noscript>
    <link
      rel="stylesheet"
      type="text/css"
      href="/lyeka-blog/css/rose-pine.min.css"
      integrity=""
    />
  </noscript>
<link
    rel="preload"
    type="text/css"
    href="/lyeka-blog/css/toigian.min.f18ce5d96459613fd8b5e153269973c8d7855f961ddd0d077d526b4226da12bc.css"
    integrity="sha256-8Yzl2WRZYT/YteFTJplzyNeFX5Yd3Q0HfVJrQibaErw="
    as="style"
    onload="this.onload=null;this.rel='stylesheet'"
  />
  <noscript>
    <link
      rel="stylesheet"
      type="text/css"
      href="/lyeka-blog/css/toigian.min.f18ce5d96459613fd8b5e153269973c8d7855f961ddd0d077d526b4226da12bc.css"
      integrity="sha256-8Yzl2WRZYT/YteFTJplzyNeFX5Yd3Q0HfVJrQibaErw="
    />
  </noscript>

  
  <link
    rel="stylesheet"
    type="text/css"
    href="/lyeka-blog/css/light.min.75b8f6206f0794f8c7268b48c912aee56c33227a8b4fe2240046195c7e5b3ac3.css"
    integrity="sha256-dbj2IG8HlPjHJotIyRKu5WwzInqLT+IkAEYZXH5bOsM="
  />
  <link
    rel="stylesheet"
    type="text/css"
    href="/lyeka-blog/css/dark.min.7ea96dfb503e6c18ad26e70c3e10e09691c711ce3a0f47fa588981348083cb5a.css"
    integrity="sha256-fqlt+1A+bBitJucMPhDglpHHEc46D0f6WImBNICDy1o="
  />

  <script>
    
    if (
      localStorage.theme === "dark" ||
      (!("theme" in localStorage) &&
        window.matchMedia("(prefers-color-scheme: dark)").matches)
    ) {
      document.documentElement.classList.add("dark");
    } else {
      document.documentElement.classList.remove("dark");
    }
  </script>

  
  
  <script
    async
    defer
    src="/lyeka-blog/js/main.dfa968b06ebe7fb755eb156f7296445f20bd5ca0c82aa2eb755b1bfac4e2f294676471cf7670590073c364901a0dd5dc4b4c7b83f7a4a3c3b081f085c78fa1de.js"
    integrity="sha512-36losG6&#43;f7dV6xVvcpZEXyC9XKDIKqLrdVsb&#43;sTi8pRnZHHPdnBZAHPDZJAaDdXcS0x7g/eko8OwgfCFx4&#43;h3g=="
  ></script>
  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <script>
    mermaid.initialize({ startOnLoad: true, securityLevel: "loose" });
  </script>
  
  
<style>
    :root { --content-width: 48rem; }            
    @media (min-width: 1280px) { :root { --content-width: 48rem; } }   
     
     
  </style>

  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500;700&display=swap" rel="stylesheet" />
  <link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100..900;1,100..900&family=Noto+Serif+SC:wght@200..900&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=LXGW+WenKai+Mono+TC&family=Montserrat:ital,wght@0,100..900;1,100..900&family=Noto+Serif+SC:wght@200..900&family=ZCOOL+XiaoWei&display=swap" rel="stylesheet">
  <style>
  :root {
       
      --font-sans: "LXGW WenKai Mono TC", "ZCOOL XiaoWei", "Songti SC", "SimSun", "NSimSun", "Noto Serif SC", serif;
       
      --font-mono: "Berkeley Mono", ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
  }
  </style>


</head>
<body><header style="--layer: 10">
  <div
    aria-hidden="true"
    class="absolute top-[-1000px] left-0 z-[calc(var(--layer)+1)] h-[calc(1000px+var(--page-top))] w-full bg-base"
  ></div>

  <div
    class="fixed top-0 left-0 z-[var(--layer)] flex h-header-height w-full items-center border-b px-page-gutter before:absolute before:inset-0 before:z-[-1] before:bg-base [@supports(backdrop-filter:blur(0))]:before:bg-base [@supports(backdrop-filter:blur(0))]:before:backdrop-blur-md"
  ><div
  class="lowercase italic mx-auto flex w-full max-w-content items-center justify-between"
>
  <nav class="mr-3">
    <ol class="flex text-md items-center">
      
  
    
  
    
  
  
  
  <li>
    
    
    <a href="/lyeka-blog/" class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">Lyeka</a>
  </li>
  
    <li class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">/</li>
  

  
  
  
  <li>
    
    
    <a href="/lyeka-blog/posts/" class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">Posts</a>
  </li>
  
    <li class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">/</li>
  

  
  
  
    
  
  <li>
    
    
    <a href="/lyeka-blog/posts/deep-research/" class="focus:underline focus:outline-none font-bold pr-1 hidden md:block">Deep Research 项目研究</a>
  </li>
  

    </ol>
  </nav>
  <nav class="ml-3">
    <ul class="flex text-md whitespace-nowrap">
      
        <li class="pl-2">
          <a href="/lyeka-blog/reading" class="tracking-wide hover:underline"
            >/Read</a
          >
        </li>
      
        <li class="pl-2">
          <a href="/lyeka-blog/posts" class="tracking-wide hover:underline"
            >/Posts</a
          >
        </li>
      
        <li class="pl-2">
          <a href="/lyeka-blog/about" class="tracking-wide hover:underline"
            >/About</a
          >
        </li>
      
    </ul>
  </nav>
</div>


</div>
</header>
<main class="flex justify-center px-page-gutter py-page-top">
      <div class="min-h-content w-full max-w-content space-y-10 sm:space-y-20"><div
  class="lowercase italic mx-auto flex w-full max-w-content items-center justify-between"
>
  <nav class="mr-3">
    <ol class="flex text-md items-center">
      
  
    
  
    
  
  
  
  <li>
    
    
    <a href="/lyeka-blog/" class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">Lyeka</a>
  </li>
  
    <li class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">/</li>
  

  
  
  
  <li>
    
    
    <a href="/lyeka-blog/posts/" class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">Posts</a>
  </li>
  
    <li class="text-muted focus:underline focus:outline-none pr-1 whitespace-nowrap">/</li>
  

  
  
  
    
  
  <li>
    
    
    <a href="/lyeka-blog/posts/deep-research/" class="focus:underline focus:outline-none font-bold pr-1 hidden md:block">Deep Research 项目研究</a>
  </li>
  

    </ol>
  </nav>
  <nav class="ml-3">
    <ul class="flex text-md whitespace-nowrap">
      
        <li class="pl-2">
          <a href="/lyeka-blog/reading" class="tracking-wide hover:underline"
            >/Read</a
          >
        </li>
      
        <li class="pl-2">
          <a href="/lyeka-blog/posts" class="tracking-wide hover:underline"
            >/Posts</a
          >
        </li>
      
        <li class="pl-2">
          <a href="/lyeka-blog/about" class="tracking-wide hover:underline"
            >/About</a
          >
        </li>
      
    </ul>
  </nav>
</div>


<article>
  <h1>Deep Research 项目研究</h1>
  
  <time
    datetime="2025/09/26"
    class="mr-1 text-sm italic tabular-nums text-muted"
  >
    2025/09/26&nbsp;
  </time>

  

  
    <div class="section">
      
        
        <a href="/lyeka-blog/tags/ai/" id="tag">AI</a>
      
        
        <a href="/lyeka-blog/tags/agent/" id="tag">Agent</a>
      
    </div>
  


<div id="toc">
  
    
      <details open>
        <summary>Table of contents</summary>
        <div id="toc-list"><nav id="TableOfContents">
  <ol>
    <li><a href="#langchain-open-deep-research">LangChain Open Deep Research</a>
      <ol>
        <li><a href="#架构">架构</a>
          <ol>
            <li><a href="#graph-node">Graph Node</a></li>
          </ol>
        </li>
        <li><a href="#llm">LLM</a></li>
        <li><a href="#think-tool">Think Tool</a></li>
        <li><a href="#prompt--上下文工程">Prompt &amp; 上下文工程</a></li>
      </ol>
    </li>
    <li><a href="#question">Question：</a></li>
    <li><a href="#benchmark-evaluation">Benchmark Evaluation</a>
      <ol>
        <li><a href="#亮点">亮点</a></li>
        <li><a href="#技术栈">技术栈</a></li>
      </ol>
    </li>
  </ol>
</nav></div>
      </details>
    
  
</div>
<div class="content"><p>研究对象</p>
<ul>
<li>LangChain Open Deep Research</li>
<li>ROMA Deep Research</li>
<li>Tongyi Deep Research</li>
</ul>
<h2 id="langchain-open-deep-research">LangChain Open Deep Research</h2>
<h3 id="架构">架构</h3>
<p><img src="/img/deep_research_read/1.png" alt="">
该项目将 任务划分成了3个阶段</p>
<ul>
<li>Scope：明确研究范围</li>
<li>Research：核心模块，执行研究任务</li>
<li>Write：输出最终报告</li>
</ul>
<h4 id="graph-node">Graph Node</h4>
<p><img src="/img/deep_research_read/2.png" alt=""></p>
<ul>
<li>Scope Phase 包含两个节点：
<ul>
<li>clarify_with_user node： 当用户的任务比较模糊，缺失足够的上下文信息时，会和用户确认收集额外信息，以得到一个相对清晰的任务。</li>
<li>write_research_brief node： 根据与用户之前的交互（包括问题澄清，在先前的研究报告上进一步探索等）生成一个研究简报。生成简报主要是为了减少噪音，整合上下文，后续的研究过程会在这个简报上进行。</li>
</ul>
</li>
<li>Research Phase 这是一个 multi agent 的实现：
<ul>
<li>Supervisor Sub Graph：确定研究简报是否可以分解为独立的子主题，并将任务分配给具有独立上下文窗口的子代理
<ul>
<li>supervisor node</li>
<li>supervisor_tools node：工具节点，包含下面工具
<ul>
<li>Think Tool：反思工具，在执行 ConductResearch 前会进行思考现在有了什么信息，还需要哪些信息等，用于规划后续 ConductResearch 行为</li>
<li>ConductResearch Tool ：调用 Research Sub-Agents  进行检索与研究，支持并发调以调研不同的 topic</li>
<li>ResearchComplete Tool：虚拟工具，作为 Research Phase 终结的条件（除了该工具，还有研究迭代次数限制，无工具调用工具会终止 Research 流程）</li>
</ul>
</li>
</ul>
</li>
<li>Researcher Sub Graph ：根据 supervisor 分配的主题进行研究
<ul>
<li>researcher node ：</li>
<li>researcher_tools node: 工具节点，包含下面工具
<ul>
<li>Think Tool： 和 Supervisor 类似，也有一个 ThinkTool 来规划后续 search 流程</li>
<li>Search Tools：搜索工具，内部支持了 TAVILY 搜索， OpenAI 和ANTHROPIC 的 Web Search 功能</li>
<li>MCP Tools：支持通过 MCP 协议动态扩展 Researcher 的工具与能力</li>
<li>ResearchComplete Tool： 作为 Researcher  终结的条件</li>
</ul>
</li>
<li>compress_research_node:  当子代理完成任务后，会将收集到的信息进行进一步撰写详细且干净的回答返回给 supervisor</li>
</ul>
</li>
</ul>
</li>
<li>Write Phase
<ul>
<li>final_report_generation node：当 supervisor 收集到足够的信息结束研究任务后，该节点结合研究简报撰写最后输出的报告</li>
</ul>
</li>
</ul>
<h3 id="llm">LLM</h3>
<p>该项目支持了 OpenAI、ANTHROPIC  和 Google 系列的 LLM（也可以扩展使用 Open Router 或者本地模型） ，同时允许在不同的节点配置使用不同的模型去处理任务
默认配置如下</p>
<ul>
<li><strong>Summarization</strong>： <code>openai:gpt-4.1-mini</code></li>
<li><strong>Research</strong>： <code>openai:gpt-4.1</code></li>
<li><strong>Compression</strong>： <code>openai:gpt-4.1</code></li>
<li><strong>Final Report Model</strong>：<code>openai:gpt-4.1</code></li>
</ul>
<p>可以根据不同阶段的任务特性选择不同的模型，例如 Research 阶段选择推理能力更好的模型，报告撰写选择文字能力更好的模型等。</p>
<h3 id="think-tool">Think Tool</h3>
<p>这里介绍一下 Think Tool，它运用在 supervisor 和 researcher node 中。
Think Tool 是在 Agent 执行任务过程中，添加一个停下来思考是否拥有继续前进所需全部信息的步骤。这在执行长串工具调用或与用户进行长时间多步骤对话时特别有帮助。它的原理类似人类反思的操作，思考目前已有的信息、之前执行过的操作与目标任务之前的偏差，决策或者纠正后续的动作。
Think Tool  虽然名称是叫工具，但它并不像其它工具（如搜索工具）一样执行获取信息等额外操作。本质上它只是将反思想法的想法添加到 LLM 的上下文内，引导后续的 LLM 决策。</p>
<p>Prompt 中用于指引 LLM 思考的部分</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="cl"><span class="o">&lt;</span><span class="n">Show</span> <span class="n">Your</span> <span class="n">Thinking</span><span class="o">&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">After</span> <span class="n">each</span> <span class="n">search</span> <span class="k">tool</span> <span class="n">call</span><span class="p">,</span> <span class="n">use</span> <span class="n">think_tool</span> <span class="n">to</span> <span class="n">analyze</span> <span class="n">the</span> <span class="n">results</span><span class="p">:</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span> <span class="n">What</span> <span class="n">key</span> <span class="n">information</span> <span class="n">did</span> <span class="n">I</span> <span class="n">find</span><span class="err">?</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span> <span class="n">What</span><span class="s1">&#39;s missing?</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span> <span class="n">Do</span> <span class="n">I</span> <span class="n">have</span> <span class="n">enough</span> <span class="n">to</span> <span class="n">answer</span> <span class="n">the</span> <span class="n">question</span> <span class="n">comprehensively</span><span class="err">?</span>
</span></span><span class="line"><span class="cl"><span class="o">-</span> <span class="n">Should</span> <span class="n">I</span> <span class="n">search</span> <span class="n">more</span> <span class="ow">or</span> <span class="n">provide</span> <span class="n">my</span> <span class="n">answer</span><span class="err">?</span>
</span></span><span class="line"><span class="cl"><span class="o">&lt;/</span><span class="n">Show</span> <span class="n">Your</span> <span class="n">Thinking</span><span class="o">&gt;</span>
</span></span></code></pre></div><p>更多关于  Think Tool 的信息可以参考 <a href="https://www.anthropic.com/engineering/claude-think-tool"># The &ldquo;think&rdquo; tool: Enabling Claude to stop and think in complex tool use situations</a> 这篇文章，里面提到了使用  Think Tool  的收益以及怎么怎么使用 Think Tool 。</p>
<h3 id="prompt--上下文工程">Prompt &amp; 上下文工程</h3>
<p>该项目使用的 Prompt 都在 <a href="https://github.com/langchain-ai/open_deep_research/blob/main/src/open_deep_research/prompts.py">prompts</a> 这个文件，这里不展开描述。</p>
<p>上下文隔离
Multi Agent 除了可以并发进行研究任务提高效率外，另外一个好处就是隔离了上下文，这是一个常见的有效管理上下文的手段。每个 Sub Agent 在给定的 topic 下工作，并不携带主线程之间的冗余信息，sub agent 有更大更聚焦的上下文专注自身任务的执行。</p>
<p>上下文压缩
在生产简报这一步其实算是上下文压缩的一种策略，其移除了与用户的对话历史，让研究任务只关注于这份简报。
Research sub agent 在任务结束后，还会对过程信息进行压缩返回给到 supervisor 节点以避免  supervisor 上下文过度膨胀。</p>
<p>上下文剪枝
当上下文超过 LLM context 限制时，agent 会尝试下面这些剪枝策略缩减上下文来重新执行任务</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># compress_research 阶段如果上下文超长则截断消息历史记录，删除到最后一条AI消息。</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_token_limit_exceeded</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">configurable</span><span class="o">.</span><span class="n">research_model</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">researcher_messages</span> <span class="o">=</span> <span class="n">remove_up_to_last_ai_message</span><span class="p">(</span><span class="n">researcher_messages</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">    <span class="k">continue</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">remove_up_to_last_ai_message</span><span class="p">(</span><span class="n">messages</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">MessageLikeRepresentation</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">MessageLikeRepresentation</span><span class="p">]:</span>  
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;Truncate message history by removing up to the last AI message.  
</span></span></span><span class="line"><span class="cl"><span class="s2">        This is useful for handling token limit exceeded errors by removing recent context.  
</span></span></span><span class="line"><span class="cl"><span class="s2">        Args:  
</span></span></span><span class="line"><span class="cl"><span class="s2">        messages: List of message objects to truncate            Returns:  
</span></span></span><span class="line"><span class="cl"><span class="s2">        Truncated message list up to (but not including) the last AI message    &#34;&#34;&#34;</span>    <span class="c1"># Search backwards through messages to find the last AI message    for i in range(len(messages) - 1, -1, -1):  </span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">messages</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">AIMessage</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">            <span class="c1"># Return everything up to (but not including) the last AI message  </span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">messages</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span>  
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="c1"># No AI messages found, return original list  </span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">messages</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># final_report_generation 阶段每次尝试删减 10% 的上下文</span>
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="n">is_token_limit_exceeded</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">configurable</span><span class="o">.</span><span class="n">final_report_model</span><span class="p">):</span>  
</span></span><span class="line"><span class="cl">    <span class="n">current_retry</span> <span class="o">+=</span> <span class="mi">1</span>  
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">current_retry</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># First retry: determine initial truncation limit  </span>
</span></span><span class="line"><span class="cl">        <span class="n">model_token_limit</span> <span class="o">=</span> <span class="n">get_model_token_limit</span><span class="p">(</span><span class="n">configurable</span><span class="o">.</span><span class="n">final_report_model</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="n">model_token_limit</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="p">{</span>  
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;final_report&#34;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&#34;Error generating final report: Token limit exceeded, however, we could not determine the model&#39;s maximum context length. Please update the model map in deep_researcher/utils.py with this information. </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">,</span>  
</span></span><span class="line"><span class="cl">                <span class="s2">&#34;messages&#34;</span><span class="p">:</span> <span class="p">[</span><span class="n">AIMessage</span><span class="p">(</span><span class="n">content</span><span class="o">=</span><span class="s2">&#34;Report generation failed due to token limits&#34;</span><span class="p">)],</span>  
</span></span><span class="line"><span class="cl">                <span class="o">**</span><span class="n">cleared_state</span>  
</span></span><span class="line"><span class="cl">            <span class="p">}</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># Use 4x token limit as character approximation for truncation  </span>
</span></span><span class="line"><span class="cl">        <span class="n">findings_token_limit</span> <span class="o">=</span> <span class="n">model_token_limit</span> <span class="o">*</span> <span class="mi">4</span>  
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>  
</span></span><span class="line"><span class="cl">        <span class="c1"># Subsequent retries: reduce by 10% each time  </span>
</span></span><span class="line"><span class="cl">        <span class="n">findings_token_limit</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">findings_token_limit</span> <span class="o">*</span> <span class="mf">0.9</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">    <span class="c1"># Truncate findings and retry  </span>
</span></span><span class="line"><span class="cl">    <span class="n">findings</span> <span class="o">=</span> <span class="n">findings</span><span class="p">[:</span><span class="n">findings_token_limit</span><span class="p">]</span>  
</span></span><span class="line"><span class="cl">    <span class="k">continue</span>
</span></span></code></pre></div><p>目前该项目没有使用到长期（外部）记忆 机制。Deep Research 这类项目目前基本以对话加一次性任务的形式呈现，没有太多引入长期记忆的需求。如果做成平台，需要结合用户偏好、历史报告等特性时才有接入长期记忆的必要性。</p>
<h2 id="question">Question：</h2>
<ul>
<li>各种类型Message（System、AI、Human、Tool、Chat &hellip;) 的区别以及使用场景？</li>
</ul>
<h2 id="benchmark-evaluation">Benchmark Evaluation</h2>
<h3 id="亮点">亮点</h3>
<h3 id="技术栈">技术栈</h3>
</div><div class="flex items-center justify-center" style="min-height:120px;">
    <p>-- End --</p>
  </div>
  
    <div class="mt-16">

</div>
  
</article>
      </div>
    </main><footer class="fixed left-0 bottom-0 w-full bg-base px-page-gutter">
  <div class="flex justify-center items-center h-footer-height">
    <div
      class="flex flex-row w-full max-w-content justify-between lowercase italic gap-3"
    >
      <nav>
        <a href="https://lyeka.github.io/lyeka-blog//index.xml"
          ><svg
            width="18px"
            height="18px"
            xmlns="http://www.w3.org/2000/svg"
            fill="none"
            viewBox="0 0 24 24"
            stroke-width="1.5"
            stroke="currentColor"
          >
            <path
              stroke-linecap="round"
              stroke-linejoin="round"
              d="M12.75 19.5v-.75a7.5 7.5 0 00-7.5-7.5H4.5m0-6.75h.75c7.87 0 14.25 6.38 14.25 14.25v.75M6 18.75a.75.75 0 11-1.5 0 .75.75 0 011.5 0z"
            />
          </svg>
        </a>
      </nav>
      <span class="text-muted hidden md:block">
        &copy;
        2025 Mingheng Lu
        Powered by 
        <a
          href="https://github.com/ntk148v/hugo-toigian"
          class="font-bold hover:underline"
          >hugo-toigian</a
        >
      </span>      
      <div id="header-theme-button">
        <svg
          id="dark_mode_btn"
          class="hidden toolbox-btn"
          width="18px"
          height="18px"
          xmlns="http://www.w3.org/2000/svg"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M21.752 15.002A9.718 9.718 0 0118 15.75c-5.385 0-9.75-4.365-9.75-9.75 0-1.33.266-2.597.748-3.752A9.753 9.753 0 003 11.25C3 16.635 7.365 21 12.75 21a9.753 9.753 0 009.002-5.998z"
          />
        </svg>
        <svg
          id="light_mode_btn"
          class="hidden toolbox-btn"
          width="18px"
          height="18px"
          xmlns="http://www.w3.org/2000/svg"
          fill="none"
          viewBox="0 0 24 24"
          stroke-width="1.5"
          stroke="currentColor"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            d="M12 3v2.25m6.364.386l-1.591 1.591M21 12h-2.25m-.386 6.364l-1.591-1.591M12 18.75V21m-4.773-4.227l-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 3.75 0 11-7.5 0 3.75 3.75 0 017.5 0z"
          />
        </svg>
      </div>
    </div>
  </div>
</footer>
</body>
</html>
